<?xml version='1.0' encoding='utf-8'?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" epub:prefix="z3998: http://www.daisy.org/z3998/2012/vocab/structure/#" lang="en" xml:lang="en">
  <head>
    <title>A. DIGITAL AUDIO</title>
  </head>
  <body><h1>A. DIGITAL AUDIO</h1><p>At a purely physical level, sound is simply a mechanical
disturbance of a medium. The medium in question may be air, solid,
liquid, gas or a combination of several of these. This disturbance
in the medium causes molecules to move back and forth in a
spring-like manner. As one molecule hits the next, the disturbance
moves through the medium causing sound to travel. These so called
compressions and rarefactions in the medium can be described as
sound waves. The simplest type of waveform, describing what is
referred to as 'simple harmonic motion', is a sine wave.</p>
<p class="aloha-empty-paragraph"/><div class="group_img" style="text-align: justify;"><div class="image bk-image-editor" style="width: 525.403px; height: 321.312px;"><img style="width: 525.403px; height: 321.312px; transform: translate(0px, 0px) rotate(0deg) scaleX(1) scaleY(1); filter: contrast(100%) brightness(100%) blur(0px) opacity(100%) saturate(100%);" src="static/csound-picts-01_basics-sinewave-en.png" alt="SineWave" transform-data="{&quot;imageWidth&quot;:525.4031781140861,&quot;imageHeight&quot;:321.3115087310827,&quot;imageTranslateX&quot;:0,&quot;imageTranslateY&quot;:0,&quot;imageScaleX&quot;:1,&quot;imageScaleY&quot;:1,&quot;imageRotateDegree&quot;:0,&quot;imageContrast&quot;:100,&quot;imageBrightness&quot;:100,&quot;imageBlur&quot;:0,&quot;imageSaturate&quot;:100,&quot;imageOpacity&quot;:100,&quot;frameWidth&quot;:525.4031781140861,&quot;frameHeight&quot;:321.3115087310827,&quot;frameFPI&quot;:false,&quot;editorWidth&quot;:898}" width="502" height="307"/></div></div><p class="aloha-empty-paragraph"/>
<p>Each time the waveform signal goes above 0 the molecules are in
a state of compression meaning that each molecule within the
waveform disturbance is pushing into its neighbour. Each time the
waveform signal drops below 0 the molecules are in a state of
rarefaction meaning the molecules are pulling away from thier
neighbours. When a waveform shows a clear repeating pattern, as in
the case above, it is said to be periodic. Periodic sounds give
rise to the sensation of pitch.</p>
<h2>Elements of a Sound Wave</h2>
<p>Periodic waves have four common parameters, and each of the four
parameters affects the way we perceive sound.</p>
<ul><li><p><strong>Period</strong>: This is the length of
time it takes for a waveform to complete one cycle. This amount of
time is referred to as <em>t</em></p></li><li><p><strong>Wavelength</strong>: the distance it
takes for a wave to complete one full period. This is usually
measured in meters.</p></li><li><p><strong>Frequency</strong>: the number of cycles
or periods per second. Frequency is measured in Hertz. If a sound
has a frequency of 440Hz it completes 440 cycles every second.
Given a frequency, one can easily calculate the period of any
sound. Mathematically, the period is the reciprocal of the
frequency (and vice versa). In equation form this is expressed as
follows.</p><pre> Frequency = 1/Period         Period = 1/Frequency
</pre></li></ul>
<div>
<p>Therefore the frequency is the inverse of the
period, so a wave of 100 Hz frequency has a period of 1/100 or 0.01
secs, likewise a frequency of 256Hz has a period of 1/256, or 0.004
secs. To calculate the wavelength of a sound in any given medium we
can use the following equation:</p>
</div>
<pre><em> Wavelength = Velocity/Frequency</em>
</pre>
<p>Humans can hear frequencies from 20Hz to 20000Hz
(although this can differ dramatically from individual to
individual and the upper limit will decay with age). You can read
more about frequency in the <a href="b-pitch-and-frequency">next chapter</a>.</p>
<ul><li><p><strong>Phase:</strong> This is the starting point of a
waveform. The starting point along the Y-axis of our plotted
waveform is not always zero. This can be expressed in degrees or in
radians. A complete cycle of a waveform will cover 360 degrees or
(2 x pi) radians.</p></li><li><p><strong>Amplitude:</strong> Amplitude is
represented by the y-axis of a plotted pressure wave. The strength
at which the molecules pull or push away from each other, which
will also depend upon the resistance offered by the medium, will
determine how far above and below zero - the point of equilibrium -
the wave fluctuates. The greater the y-value the greater the
amplitude of our wave. The greater the compressions and
rarefactions, the greater the amplitude.</p></li></ul>
<h2>Transduction</h2>
<p>The analogue sound waves we hear in the world around us need to
be converted into an electrical signal in order to be amplified or
sent to a soundcard for recording. The process of converting
acoustical energy in the form of pressure waves into an electrical
signal is carried out by a device known as a a transducer.</p>
<p>A transducer, which is usually found in microphones, produces a
changing electrical voltage that mirrors the changing compression
and rarefaction of the air molecules caused by the sound wave. The
continuous variation of pressure is therefore 'transduced' into
continuous variation of voltage. The greater the variation of
pressure the greater the variation of voltage that is sent to the
computer.</p>
<p>Ideally the transduction process should be as transparent as
possible: whatever goes in should come out as a perfect analogue in
a voltage representation. In reality however this will not be the
case, noise and distortion are always incorporated into the signal.
Every time sound passes through a transducer or is transmitted
electrically a change in signal quality will result. When we talk
of 'noise' we are talking specifically about any unwanted signal
captured during the transduction process. This normally manifests
itself as an unwanted 'hiss'.</p>
<p> </p>
<h2>Sampling</h2>
<p>The analogue voltage that corresponds to an acoustic signal
changes continuously so that at each instant in time it will have a
different value. It is not possible for a computer to receive the
value of the voltage for every instant because of the physical
limitations of both the computer and the data converters (remember
also that there are an infinite number of instances between every
two instances!).</p>
<p>What the soundcard can do, however, is to measure the power of
the analogue voltage at intervals of equal duration. This is how
all digital recording works and this is known as 'sampling'. The
result of this sampling process is a discrete, or digital, signal
which is no more than a sequence of numbers corresponding to the
voltage at each successive moment of sampling.</p>
<p>Below is a diagram showing a sinusoidal waveform. The
vertical lines that run through the diagram represent the points
in time when a snapshot is taken of the signal. After the sampling
has taken place we are left with what is known as a discrete signal
consisting of a collection of audio samples, as illustrated in the
bottom half of the diagram. If one is recording using a
typical audio editor the incoming samples will be stored in the
computer's RAM (Random Access Memory). In Csound one can process
the incoming audio samples in real time and output a new stream of
samples or write them to disk in the form of a sound file.</p><div class="group_img image-layout-1image_1caption_bottom" style="text-align: start;"><div class="image bk-image-editor" style="width: 563.472px; height: 343.937px;"><img style="width: 563.472px; height: 343.937px; transform: translate(0px, 0px) rotate(0deg) scaleX(1) scaleY(1); filter: contrast(100%) brightness(100%) blur(0px) opacity(100%) saturate(100%);" src="static/01a_sampling_1b.png" alt="" transform-data="{&quot;imageWidth&quot;:563.4714784633295,&quot;imageHeight&quot;:343.9371362048894,&quot;imageTranslateX&quot;:0,&quot;imageTranslateY&quot;:0,&quot;imageScaleX&quot;:1,&quot;imageScaleY&quot;:1,&quot;imageRotateDegree&quot;:0,&quot;imageContrast&quot;:100,&quot;imageBrightness&quot;:100,&quot;imageBlur&quot;:0,&quot;imageSaturate&quot;:100,&quot;imageOpacity&quot;:100,&quot;frameWidth&quot;:563.4714784633295,&quot;frameHeight&quot;:343.9371362048894,&quot;frameFPI&quot;:false,&quot;editorWidth&quot;:898}"/></div><div class="caption_small" style="width: 563.472px;">Sampling of an analog signal<br/></div></div>
<p class="aloha-empty-paragraph"/>
<p>It is important to remember that each sample represents the
amount of voltage, positive or negative, that was present in the
signal at the point in time at which the sample or snapshot was
taken.</p>
<p>The same principle applies to recording of live video: a video
camera takes a sequence of pictures of motion and most video
cameras will take between 30 and 60 still pictures a second. Each
picture is called a frame and when these frames are played in
sequence at a rate corresponding to that at which they were taken
we no longer perceive them as individual pictures, we perceive them
instead as a continuous moving image.</p>
<h2>Analogue versus Digital</h2>
<p>In general, analogue systems can be quite unreliable when it
comes to noise and distortion. Each time something is copied or
transmitted some noise and distortion is introduced into the
process. If this is repeated many times, the cumulative effect can
deteriorate a signal quite considerably. It is for this reason that
the music industry has almost entirely turned to digital
technology. One particular advantage of storing a signal digitally
is that once the changing signal has been converted to a discrete
series of values, it can effectively be 'cloned' an clones can be
made of that clone with no loss or distortion of data. Mathematical
routines can be applied to prevent errors in transmission, which
could otherwise introduce noise into the signal.</p>
<h2>Sample Rate and the Sampling Theorem</h2>
<p>The sample rate describes the number of samples
(pictures/snapshots) taken each second. To sample an audio signal
correctly it is important to pay attention to the sampling
theorem:</p>
<br/><p style=""><em>To represent digitally a signal containing frequencies up to X Hz, it is necessary to use a sampling rate of at least 2X samples per second.</em></p>
<p>According to this theorem, a soundcard or any other digital
recording device will not be able to represent any frequency above
1/2 the sampling rate. Half the sampling rate is also referred to
as the Nyquist frequency, after the Swedish physicist Harry Nyquist
who formalized the theory in the 1920s. What it all means is that
any signal with frequencies above the Nyquist frequency will be
misrepresented and will actually produce a frequency lower than the
one being sampled. When this happens it results in what is known as
'aliasing' or 'foldover'.</p>
<h2>Aliasing</h2>
<p>Here is a graphical representation of aliasing.</p>
<div class="group_img image-layout-1image_1caption_bottom" style="text-align: start;"><div class="image bk-image-editor" style="width: 544.654px; height: 343.937px;"><img style="width: 544.654px; height: 343.937px; transform: translate(0px, 0px) rotate(0deg) scaleX(1) scaleY(1); filter: contrast(100%) brightness(100%) blur(0px) opacity(100%) saturate(100%);" src="static/01a_aliasing_1b.png" alt="" transform-data="{&quot;imageWidth&quot;:544.6542491268917,&quot;imageHeight&quot;:343.9371362048894,&quot;imageTranslateX&quot;:0,&quot;imageTranslateY&quot;:0,&quot;imageScaleX&quot;:1,&quot;imageScaleY&quot;:1,&quot;imageRotateDegree&quot;:0,&quot;imageContrast&quot;:100,&quot;imageBrightness&quot;:100,&quot;imageBlur&quot;:0,&quot;imageSaturate&quot;:100,&quot;imageOpacity&quot;:100,&quot;frameWidth&quot;:544.6542491268917,&quot;frameHeight&quot;:343.9371362048894,&quot;frameFPI&quot;:false,&quot;editorWidth&quot;:898}"/></div><div class="caption_small" style="width: 544.654px;">Aliasing (red) of a high frequency (blue)<br/></div></div><p class="aloha-editing-p" style=""><br/>
The sinusoidal waveform in blue is being sampled at the vertical black lines. The line that joins the red circles
together is the captured waveform. As you can see, the captured
waveform and the original waveform express different
frequencies.</p>
<p>Here is another example:</p><div class="group_img image-layout-1image_1caption_bottom" style="text-align: start;"><div class="image bk-image-editor" style="width: 610px; height: 360px;"><img style="width: 610px; height: 360px; transform: translate(0px, 0px) rotate(0deg) scaleX(1) scaleY(1); filter: contrast(100%) brightness(100%) blur(0px) opacity(100%) saturate(100%);" src="static/01a_aliasing_2.png" alt="" transform-data="{&quot;imageWidth&quot;:610.0002142025612,&quot;imageHeight&quot;:359.9997322467986,&quot;imageTranslateX&quot;:0,&quot;imageTranslateY&quot;:0,&quot;imageScaleX&quot;:1,&quot;imageScaleY&quot;:1,&quot;imageRotateDegree&quot;:0,&quot;imageContrast&quot;:100,&quot;imageBrightness&quot;:100,&quot;imageBlur&quot;:0,&quot;imageSaturate&quot;:100,&quot;imageOpacity&quot;:100,&quot;frameWidth&quot;:610.0002142025612,&quot;frameHeight&quot;:359.9997322467986,&quot;frameFPI&quot;:false,&quot;editorWidth&quot;:898}"/></div><div class="caption_small" style="width: 610px;">Aliasing of a 30 kHz sine at 40 kHz sample rate<br/></div></div>
<p class="aloha-empty-paragraph"/>
<p>We can see that if the sample rate is 40,000 there is no problem
with sampling a signal that is 10KHz. On the other hand, in the
second example it can be seen that a 30kHz waveform is not going to
be correctly sampled. In fact we end up with a waveform that is
10kHz, rather than 30kHz. This may seem like an academic
proposition in that we will never be able to hear a 30KHz waveform
anyway but some synthesis and DSP techniques procedures will
produce these frequencies as unavoidable by-products and we need to
ensure that they do not result in unwanted artifacts. </p>
<p>The following Csound instrument plays a 1000 Hz tone first
directly, and then because the frequency is 1000 Hz lower than the
sample rate of 44100 Hz:</p><p><br/></p>
<p><em><strong>EXAMPLE 01A01_Aliasing.csd</strong></em></p>
<pre>&lt;CsoundSynthesizer&gt;
&lt;CsOptions&gt;
-odac
&lt;/CsOptions&gt;
&lt;CsInstruments&gt;
;example by Joachim Heintz
sr = 44100
ksmps = 32
nchnls = 2
0dbfs = 1

instr 1
 asig poscil .2, p4
 out asig, asig
endin

&lt;/CsInstruments&gt;
&lt;CsScore&gt;
i 1 0 2 1000 ;1000 Hz tone
i 1 3 2 43100 ;43100 Hz tone sounds like 1000 Hz because of aliasing
&lt;/CsScore&gt;
&lt;/CsoundSynthesizer&gt;
</pre>
<p>The same phenomenon takes places in film and video too. You may
recall having seen wagon wheels apparently turn at the wrong speed
in old Westerns. Let us say for example that a camera is taking 60
frames per second of a wheel moving. In one example, if the wheel
is completing one rotation in exactly 1/60th of a second, then
every picture looks the same and as a result the wheel appears to
be motionless. If the wheel speeds up, i.e. it increases its
rotational frequency, it will appear as if the wheel is slowly
turning backwards. This is because the wheel will complete more
than a full rotation between each snapshot.</p>
<p>As an aside, it is worth observing that a lot of modern 'glitch'
music intentionally makes a feature of the spectral distortion that
aliasing induces in digital audio. Csound is perfectly capable of
imitating the effects of aliasing while being run at any sample
rate - if that is what you desire.</p>
<p>Audio-CD Quality uses a sample rate of 44100Kz (44.1 kHz). This
means that CD quality can only represent frequencies up to 22050Hz.
Humans typically have an absolute upper limit of hearing of about
20Khz thus making 44.1KHz a reasonable standard sampling rate.</p>
<h2>Bits, Bytes and Words. Understanding Binary.</h2>
<p>All digital computers represent data as a collection of bits
(short for binary digit). A bit is the smallest possible unit of
information. One bit can only be in one of two states - off or on,
0 or 1. The meaning of the bit - which can represent almost
anything - is unimportant at this point, the thing to remember is
that all computer data - a text file on disk, a program in memory,
a packet on a network - is ultimately a collection of bits.</p>
<p>Bits in groups of eight are called bytes, and one byte usually
represents a single character of data in the computer. It's a
little used term, but you might be interested in knowing that a
nibble is half a byte (usually 4 bits).</p>
<p> </p>
<h2>The Binary System</h2>
<p>All digital computers work in a environment that has only two
variables, 0 and 1. All numbers in our decimal system therefore
must be translated into 0's and 1's in the binary system. If you
think of binary numbers in terms of switches. With one switch you
can represent up to two different numbers.</p>
<p>0 (OFF) = Decimal 0<br/>
1 (ON) = Decimal 1</p>
<p>Thus, a single bit represents 2 numbers, two bits can represent
4 numbers, three bits represent 8 numbers, four bits represent 16
numbers, and so on up to a byte, or eight bits, which represents
256 numbers. Therefore each added bit doubles the amount of
possible numbers that can be represented. Put simply, the more bits
you have at your disposal the more information you can store.</p>
<p> </p>
<h2>Bit-depth Resolution</h2>
<p>Apart from the sample rate, another important attribute that can
affect the fidelity of a digital signal is the accuracy with which
each sample is known, its resolution or granularity. Every sample
obtained is set to a specific amplitude (the measure of strength
for each voltage) level. Each voltage measurement will probably
have to be rounded up or down to the nearest digital value
available. The number of levels available depends on the precision
of the measurement in bits, i.e. how many binary digits are used to
store the samples. The number of bits that a system can use is
normally referred to as the bit-depth resolution.</p>
<p style="">If the bit-depth resolution is 3 then there are 8 possible
levels of amplitude that we can use for each sample. We can see
this in the diagram below. At each sampling period the soundcard
plots an amplitude. As we are only using a 3-bit system the
resolution is not good enough to plot the correct amplitude of each
sample. We can see in the diagram that some vertical lines stop
above or below the real signal. For a signal with lower amplitude the distortion would even be stronger.<br/></p><div class="group_img image-layout-1image_1caption_bottom" style="text-align: start;"><div class="image bk-image-editor" style="width: 898px; height: 367.981px;"><img style="width: 898px; height: 367.981px; transform: translate(0px, 0px) rotate(0deg) scaleX(1) scaleY(1); filter: contrast(100%) brightness(100%) blur(0px) opacity(100%) saturate(100%);" src="static/01a_bitdepth.png" alt="" transform-data="{&quot;imageWidth&quot;:898,&quot;imageHeight&quot;:367.9813736903376,&quot;imageTranslateX&quot;:0,&quot;imageTranslateY&quot;:0,&quot;imageScaleX&quot;:1,&quot;imageScaleY&quot;:1,&quot;imageRotateDegree&quot;:0,&quot;imageContrast&quot;:100,&quot;imageBrightness&quot;:100,&quot;imageBlur&quot;:0,&quot;imageSaturate&quot;:100,&quot;imageOpacity&quot;:100,&quot;frameWidth&quot;:898,&quot;frameHeight&quot;:367.9813736903376,&quot;frameFPI&quot;:false,&quot;editorWidth&quot;:898}"/></div><div class="caption_small" style="width: 898px;">Wrong amplitude values due to insufficient bit depth resolution<br/></div></div>
<p class="aloha-empty-paragraph"/><p class="aloha-empty-paragraph"/>
<p>The standard resolution for CDs is 16 bit, which allows for
65536 different possible amplitude levels, 32767 either side of the
zero axis. Using bit rates lower than 16 is not a good idea as it
will result in noise being added to the signal. This is referred to
as quantization noise and is a result of amplitude values being
excessively rounded up or down when being digitized. Quantization
noise becomes most apparent when trying to represent low amplitude
(quiet) sounds. Frequently a tiny amount of noise, known as a
dither signal, will be added to digital audio before conversion
back into an analogue signal. Adding this dither signal will
actually reduce the more noticeable noise created by quantization.
As higher bit depth resolutions are employed in the digitizing
process the need for dithering is reduced. A general rule is to use
the highest bit rate available.</p>
<p>Many electronic musicians make use of deliberately low bit depth
quantization in order to add noise to a signal. The effect is
commonly known as 'bit-crunching' and is relatively easy to
implement in Csound.</p>
<h2>ADC / DAC</h2>
<p>The entire process, as described above, of taking an analogue
signal and converting it to a digital signal is referred to as
analogue to digital conversion, or ADC. Of course digital to
analogue conversion, DAC, is also possible. This is how we get to
hear our music through our PC's headphones or speakers. For
example, if one plays a sound from Media Player or iTunes the
software will send a series of numbers to the computer soundcard.
In fact it will most likely send 44100 numbers a second. If the
audio that is playing is 16 bit then these numbers will range from
-32768 to +32767.</p>
<p>When the sound card receives these numbers from the audio stream
it will output corresponding voltages to a loudspeaker. When the
voltages reach the loudspeaker they cause the loudspeaker's magnet
to move inwards and outwards. This causes a disturbance in the air
around the speaker - the compressions and rarefactions introduced
at the beginning of this chapter - resulting in what we perceive as
sound.</p>


</body>
</html>
